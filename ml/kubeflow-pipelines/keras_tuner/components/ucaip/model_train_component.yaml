name: Create training pipeline custom job
inputs:
- {name: project, type: String}
- {name: display_name, type: String}
- {name: model_display_name, type: String}
- {name: train_container_type, type: String}
- {name: executor_image_uri, type: String}
- {name: package_uri, type: String}
- {name: python_module, type: String}
- {name: container_image_uri, type: String}
- {name: base_output_directory_prefix, type: String}
- {name: prediction_image_uri, type: String}
- {name: location, type: String}
- {name: api_endpoint, type: String}
- {name: data_dir, type: String}
- {name: hptune_dict, type: String}
outputs:
- {name: model_id, type: String}
- {name: model_dispname, type: String}
implementation:
  container:
    image: gcr.io/aju-vtests2/bw-aiplatform:v1
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def create_training_pipeline_custom_job(
          project,
          display_name,
          model_display_name,
          train_container_type,
          executor_image_uri,
          package_uri,
          python_module,
          container_image_uri,
          base_output_directory_prefix,
          prediction_image_uri,  # 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest'
          location,  # "us-central1"
          api_endpoint,  # "us-central1-aiplatform.googleapis.com",
          data_dir,
          hptune_dict,
          # model_id: OutputPath('String'),
          # model_dispname: OutputPath('String')
      ):

        import logging
        import subprocess
        import time

        from google.cloud import aiplatform
        from google.protobuf import json_format
        from google.protobuf.struct_pb2 import Value
        from google.cloud.aiplatform_v1beta1.types import pipeline_state

        logging.getLogger().setLevel(logging.INFO)

        # The AI Platform services require regional API endpoints.
        client_options = {"api_endpoint": api_endpoint}
        # Initialize client that will be used to create and send requests.
        # This client only needs to be created once, and can be reused for multiple requests.
        client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)

        if train_container_type == 'prebuilt':
          python_package_spec = {
              "executor_image_uri": executor_image_uri,
              "package_uris": [package_uri],
              "python_module": python_module,
              "args": [f"--data-dir={data_dir}",
                       f"--hptune-dict={hptune_dict}"]}
          worker_pool_spec = {
                    "machine_spec": {
                        "machine_type": "n1-standard-16",
                        "accelerator_type": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,
                        "accelerator_count": 2,
                        },
                    "replica_count": 1,
                    "python_package_spec": python_package_spec,
                }
        elif train_container_type == 'custom':
          container_spec = {
              # A working docker image can be found at gs://cloud-samples-data/ai-platform/mnist_tfrecord/custom_job
              "imageUri": container_image_uri,
              "args": [
                  # AIP_MODEL_DIR is set by the service according to baseOutputDirectory.
                  "--model_dir=$(AIP_MODEL_DIR)",
              ]}
          worker_pool_spec = {
                    "machine_spec": {
                        "machine_type": "n1-standard-16",
                        "accelerator_type": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,
                        "accelerator_count": 2,
                        },
                    "replica_count": 1,
                    "container_spec": container_spec,
                }

        training_task_inputs_dict = {
            "workerPoolSpecs": [
              worker_pool_spec
            ],
            "baseOutputDirectory": {
                # The GCS location for outputs must be accessible by the project's AI Platform service account.
                "output_uri_prefix": base_output_directory_prefix
            },
        }
        training_task_inputs = json_format.ParseDict(training_task_inputs_dict, Value())

        training_task_definition = "gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml"
        # image_uri = "gcr.io/cloud-aiplatform/prediction/tf-cpu.1-15:latest"
        # image_uri = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest'

        training_pipeline = {
            "display_name": display_name,
            "training_task_definition": training_task_definition,
            "training_task_inputs": training_task_inputs,
            "model_to_upload": {
                "display_name": model_display_name,
                "container_spec": {"image_uri": prediction_image_uri},
            },
        }
        parent = f"projects/{project}/locations/{location}"
        response = client.create_training_pipeline(
            parent=parent, training_pipeline=training_pipeline
        )
        logging.info("training pipeline request response: %s", response)

        SLEEP_INTERVAL = 100

        training_pipeline_name = response.name
        logging.info("training pipeline name: %s", training_pipeline_name)
        # Poll periodically until training completes
        while True:
          mresponse = client.get_training_pipeline(name=training_pipeline_name)
          logging.info('mresponse: %s', mresponse)
          logging.info('job state: %s', mresponse.state)
          if mresponse.state == pipeline_state.PipelineState.PIPELINE_STATE_FAILED:
            logging.warning('training pipeline failed: %s', mresponse)
            break
          if mresponse.state == pipeline_state.PipelineState.PIPELINE_STATE_SUCCEEDED:
            logging.info('training finished')
            model_name = mresponse.model_to_upload.name
            return (model_name, model_display_name)
            # # write some outputs once finished
            # model_name = mresponse.model_to_upload.name
            # logging.info('got model name: %s', model_name)
            # with open('temp.txt', "w") as outfile:
            #   outfile.write(model_name)
            # subprocess.run(['gsutil', 'cp', 'temp.txt', model_id])
            # with open('temp2.txt', "w") as outfile:
            #   outfile.write(model_display_name)
            # subprocess.run(['gsutil', 'cp', 'temp2.txt', model_dispname])
            # break
          else:
            time.sleep(SLEEP_INTERVAL)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Create training pipeline custom job', description='')
      _parser.add_argument("--project", dest="project", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--display-name", dest="display_name", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model-display-name", dest="model_display_name", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--train-container-type", dest="train_container_type", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--executor-image-uri", dest="executor_image_uri", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--package-uri", dest="package_uri", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--python-module", dest="python_module", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--container-image-uri", dest="container_image_uri", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--base-output-directory-prefix", dest="base_output_directory_prefix", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--prediction-image-uri", dest="prediction_image_uri", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--location", dest="location", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--api-endpoint", dest="api_endpoint", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--hptune-dict", dest="hptune_dict", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = create_training_pipeline_custom_job(**_parsed_args)

      _output_serializers = [
          _serialize_str,
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --project
    - {inputValue: project}
    - --display-name
    - {inputValue: display_name}
    - --model-display-name
    - {inputValue: model_display_name}
    - --train-container-type
    - {inputValue: train_container_type}
    - --executor-image-uri
    - {inputValue: executor_image_uri}
    - --package-uri
    - {inputValue: package_uri}
    - --python-module
    - {inputValue: python_module}
    - --container-image-uri
    - {inputValue: container_image_uri}
    - --base-output-directory-prefix
    - {inputValue: base_output_directory_prefix}
    - --prediction-image-uri
    - {inputValue: prediction_image_uri}
    - --location
    - {inputValue: location}
    - --api-endpoint
    - {inputValue: api_endpoint}
    - --data-dir
    - {inputValue: data_dir}
    - --hptune-dict
    - {inputValue: hptune_dict}
    - '----output-paths'
    - {outputPath: model_id}
    - {outputPath: model_dispname}
