name: Automl eval metrics
inputs:
- name: eval_data
  type: evals
- name: thresholds
  type: String
  default: '{"mean_absolute_error": 460}'
  optional: true
- name: confidence_threshold
  type: Float
  default: '0.5'
  optional: true
outputs:
- name: mlpipeline_ui_metadata
  type: UI_metadata
- name: mlpipeline_metrics
  type: UI_metrics
- name: deploy
  type: String
implementation:
  container:
    image: python:3.7
    command:
    - python3
    - -u
    - -c
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def automl_eval_metrics(
        eval_data_path ,
        mlpipeline_ui_metadata_path ,
        mlpipeline_metrics_path ,
        # thresholds: str = '{"au_prc": 0.9}',
        thresholds  = '{"mean_absolute_error": 460}',
        confidence_threshold  = 0.5  # for classification

      # ) -> NamedTuple('Outputs', [('deploy', str)]):  # this gives the same result
      )    :
        import subprocess
        import sys
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'googleapis-common-protos==1.6.0',
            '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0',
           'google-cloud-storage',
           '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

        import google
        import json
        import logging
        import pickle

        logging.getLogger().setLevel(logging.INFO)  # TODO: make level configurable

        thresholds_dict = json.loads(thresholds)
        logging.info('thresholds dict: {}'.format(thresholds_dict))

        def regression_threshold_check(eval_info):
          eresults = {}
          rmetrics = eval_info[1].regression_evaluation_metrics
          logging.info('got regression eval {}'.format(eval_info[1]))
          eresults['root_mean_squared_error'] = rmetrics.root_mean_squared_error
          eresults['mean_absolute_error'] = rmetrics.mean_absolute_error
          eresults['r_squared'] = rmetrics.r_squared
          eresults['mean_absolute_percentage_error'] = rmetrics.mean_absolute_percentage_error
          eresults['root_mean_squared_log_error'] = rmetrics.root_mean_squared_log_error
          for k, v in thresholds_dict.items():
            logging.info('k {}, v {}'.format(k, v))
            if k in ['root_mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error']:
              if eresults[k] > v:
                logging.info('{} > {}; returning False'.format(
                    eresults[k], v))
                return ('False', eresults)
            elif eresults[k] < v:
              logging.info('{} < {}; returning False'.format(
                  eresults[k], v))
              return ('False', eresults)
          return ('deploy', eresults)

        def classif_threshold_check(eval_info):
          eresults = {}
          example_count = eval_info[0].evaluated_example_count
          print('Looking for example_count {}'.format(example_count))
          for e in eval_info[1:]:  # we know we don't want the first elt
            if e.evaluated_example_count == example_count:
              eresults['au_prc'] = e.classification_evaluation_metrics.au_prc
              eresults['au_roc'] = e.classification_evaluation_metrics.au_roc
              eresults['log_loss'] = e.classification_evaluation_metrics.log_loss
              for i in e.classification_evaluation_metrics.confidence_metrics_entry:
                if i.confidence_threshold >= confidence_threshold:
                  eresults['recall'] = i.recall
                  eresults['precision'] = i.precision
                  eresults['f1_score'] = i.f1_score
                  break
              break
          logging.info('eresults: {}'.format(eresults))
          for k, v in thresholds_dict.items():
            logging.info('k {}, v {}'.format(k, v))
            if k == 'log_loss':
              if eresults[k] > v:
                logging.info('{} > {}; returning False'.format(
                    eresults[k], v))
                return ('False', eresults)
            else:
              if eresults[k] < v:
                logging.info('{} < {}; returning False'.format(
                    eresults[k], v))
                return ('False', eresults)
          return ('deploy', eresults)

        with open(eval_data_path, 'rb') as f:
          logging.info('successfully opened eval_data_path {}'.format(eval_data_path))
          try:
            eval_info = pickle.loads(f.read())

            classif = False
            regression = False
            # TODO: what's the right way to figure out the model type?
            if eval_info[1].regression_evaluation_metrics and eval_info[1].regression_evaluation_metrics.root_mean_squared_error:
              regression=True
              logging.info('found regression metrics {}'.format(
                  eval_info[1].regression_evaluation_metrics))
            elif eval_info[1].classification_evaluation_metrics and eval_info[1].classification_evaluation_metrics.au_prc:
              classif = True
              logging.info('found classification metrics {}'.format(
                  eval_info[1].classification_evaluation_metrics))

            if regression and thresholds_dict:
              res, eresults = regression_threshold_check(eval_info)
              # logging.info('eresults: {}'.format(eresults))
              metadata = {
                'outputs' : [
                {
                  'storage': 'inline',
                  'source': '# Regression metrics:\n\n```{}```\n'.format(eresults),
                  'type': 'markdown',
                }]}
              metrics = {
                'metrics': [{
                  'name': 'MAE',
                  'numberValue':  eresults['mean_absolute_error'],
                  'format': "RAW",
                }]
              }
              logging.info('using metadata dict {}'.format(json.dumps(metadata)))
              logging.info('using metadata ui path: {}'.format(mlpipeline_ui_metadata_path))
              with open(mlpipeline_ui_metadata_path, 'w') as mlpipeline_ui_metadata_file:
                mlpipeline_ui_metadata_file.write(json.dumps(metadata))
              logging.info('using metrics path: {}'.format(mlpipeline_metrics_path))
              with open(mlpipeline_metrics_path, 'w') as mlpipeline_metrics_file:
                mlpipeline_metrics_file.write(json.dumps(metrics))
              logging.info('deploy flag: {}'.format(res))
              return (res,)

            if classif and thresholds_dict:
              res, eresults = classif_threshold_check(eval_info)
              # logging.info('eresults: {}'.format(eresults))
              metadata = {
                'outputs' : [
                {
                  'storage': 'inline',
                  'source': '# classification metrics for confidence threshold {}:\n\n```{}```\n'.format(
                      confidence_threshold, eresults),
                  'type': 'markdown',
                }]}
              logging.info('using metadata dict {}'.format(json.dumps(metadata)))
              logging.info('using metadata ui path: {}'.format(mlpipeline_ui_metadata_path))
              with open(mlpipeline_ui_metadata_path, 'w') as mlpipeline_ui_metadata_file:
                mlpipeline_ui_metadata_file.write(json.dumps(metadata))
              logging.info('deploy flag: {}'.format(res))
              return (res,)
            return ('deploy',)
          except Exception as e:
            logging.warning(e)
            # If can't reconstruct the eval, or don't have thresholds defined,
            # return True as a signal to deploy.
            # TODO: is this the right default?
            return ('deploy',)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Automl eval metrics', description='')
      _parser.add_argument("--eval-data", dest="eval_data_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--thresholds", dest="thresholds", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--confidence-threshold", dest="confidence_threshold", type=float, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = automl_eval_metrics(**_parsed_args)

      _output_serializers = [
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --eval-data
    - inputPath: eval_data
    - if:
        cond:
          isPresent: thresholds
        then:
        - --thresholds
        - inputValue: thresholds
    - if:
        cond:
          isPresent: confidence_threshold
        then:
        - --confidence-threshold
        - inputValue: confidence_threshold
    - --mlpipeline-ui-metadata
    - outputPath: mlpipeline_ui_metadata
    - --mlpipeline-metrics
    - outputPath: mlpipeline_metrics
    - '----output-paths'
    - outputPath: deploy
