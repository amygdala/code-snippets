{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZEcVTiB-RNl"
   },
   "source": [
    "# Event-triggered Kubeflow Pipeline runs, and using TFDV to detect data drift\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VewejE_mC6rV"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "With ML workflows, it is often insufficient to train and deploy a given model just once.  Even if the model has desired accuracy initially, this can change if the data used for making prediction requests becomes— perhaps over time— sufficiently different from the data used to originally train the model.  \n",
    "\n",
    "This notebook shows how to build a [Kubeflow Pipeline](https://www.kubeflow.org/docs/pipelines/) that checks for statistical *drift* across successive versions of a dataset and uses that information to make a decision on whether to (re)train a model; and how to configure event-driven deployment of pipeline jobs when new data arrives.\n",
    "\n",
    "The example builds on a [previous example](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md) and introduces two primary new concepts:\n",
    "\n",
    "- it demonstrates use of the [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/guide/tfdv) library to build pipeline *components* that derive dataset **statistics** and detect **drift** between older and newer datasets, and shows how to use drift information to decide whether to retrain a model on newer data.\n",
    "- it shows how to support **event-triggered** launch of KFP Pipelines runs from a [**Cloud Functions**](https://cloud.google.com/functions/docs/) (GCF) function, where the Function run is triggered by addition of a file to a given [Cloud Storage](https://cloud.google.com/storage) (GCS) bucket.\n",
    "\n",
    "Familiarity with the previous example is not necessary for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f2fjjz8NQri"
   },
   "source": [
    "\n",
    "The machine learning task uses a tabular dataset that joins London bike rental information with weather data, and train a Keras model to predict rental `duration`. \n",
    "See [this](https://amygdala.github.io/gcp_blog/ml/kfp/kubeflow/keras/tensorflow/hp_tuning/2020/10/19/keras_tuner.html) and [this](https://amygdala.github.io/gcp_blog/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html) blog post and associated [README](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md) for more background on the dataset and model architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWACue6PW7bk"
   },
   "source": [
    "## Setup\n",
    "\n",
    "\n",
    "This notebook is intended to be run on either one of:\n",
    "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
    "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "\n",
    "\n",
    "This example requires a [Google Cloud Platform (GCP)](https://cloud.google.com/) account and project, ideally with quota for using GPUs.\n",
    "\n",
    "The example also **requires you to have installed AI Platform Pipelines (Hosted Kubeflow Pipelines)** as done from this panel of the Cloud Console: https://console.cloud.google.com/ai-platform/pipelines/, with a few additional configurations once installation is complete.   \n",
    "We'll do that first in the next section, then return to the notebook to install some libraries and set up some variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgw3SHnq4mOu"
   },
   "source": [
    "### AI Pipelines (Hosted Kubeflow Pipelines) installation and setup\n",
    "\n",
    "(**See [this README](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md#1-create-a-cloud-ai-platform-pipelines-installation) for a more detailed walkthrough of this setup process**.)\n",
    "\n",
    "Install AI Platform Pipelines (Hosted KFP) as described in the [documentation](https://cloud.google.com/ai-platform/pipelines/docs). Visit this panel in the Cloud Console to do the installation: https://console.cloud.google.com/ai-platform/pipelines/.     \n",
    "During the installation process, select \"**Create a new cluster**\", and **check the box** that says \"Allow access to the following Cloud APIs: https://www.googleapis.com/auth/cloud-platform\".\n",
    "\n",
    "The installation spins up a [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine) cluster and installs Kubeflow Pipelines onto that cluster.  \n",
    "For this example to run correctly, **some additional configuration needs to be done after the installation has completed**.  \n",
    "\n",
    "First, set up `kubectl` to use the credentials of your new GKE cluster. An easy way to do this is to click on the name of the cluster from the [Pipelines listing in the Cloud Console](https://console.cloud.google.com/ai-platform/pipelines/), then from the cluster details page, click the \"Connect\" button at the top of the page.  This will display a command that you can run to configure `kubectl` command-line access. It will look like:\n",
    "```\n",
    "gcloud container clusters get-credentials <cluster-name> --zone <zone> --project <project-id>\n",
    "```\n",
    "\n",
    "You should be able to run that command and the following ones in the notebook Terminal, or from the [Cloud Shell](https://cloud.google.com/shell/), or locally if you have the [`gcloud` SDK](https://cloud.google.com/sdk/docs/install) installed.\n",
    "\n",
    "Once you've connected to your new cluster, run:\n",
    "\n",
    "```sh\n",
    "kubectl create clusterrolebinding sa-admin --clusterrole=cluster-admin --serviceaccount=kubeflow:pipeline-runner\n",
    "```\n",
    "\n",
    "(This gives the pipeline steps permissions to create new cluster resources).\n",
    "\n",
    "Next, define a `daemonset` to install the nvidia driver on any GPU-enabled cluster nodes:\n",
    "\n",
    "```sh\n",
    "kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n",
    "```\n",
    "\n",
    "Finally, set up a **GPU node pool**, which is used (by default) for the pipeline's training step.  **Edit the following to use your values**:\n",
    "\n",
    "```sh\n",
    "gcloud container node-pools create gpu-pool2 \\\n",
    "    --cluster=<your-cluster-name> \\\n",
    "    --zone <your-cluster-zone> \\\n",
    "    --enable-autoscaling --max-nodes=3 --min-nodes=0 \\\n",
    "    --machine-type n1-highmem-8 \\\n",
    "    --scopes cloud-platform --verbosity error \\\n",
    "    --accelerator=type=nvidia-tesla-k80,count=2\n",
    "```\n",
    "\n",
    "You may need to increase your k80 quota before setting up the node pool. (If you don't want to use a GPU node pool, then later in the notebook you can alter the pipeline definition so that it is not required.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOpZ41iBW7bl"
   },
   "source": [
    "### Set some variables and install the KFP SDK\n",
    "\n",
    "Now we'll return to the notebook config.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAaCPLjgiJrO"
   },
   "source": [
    "Set `gcloud` to use your project.  **Edit the following cell before running it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GlP_C9mY3Gq"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkWdxe4TXRHk"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gckGHdW9iPrq"
   },
   "source": [
    "If you're running this notebook on colab, authenticate with your user account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZQA0KrfXCvU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpdfRA4vW7bq"
   },
   "source": [
    "Then, install the KFP SDK and (on AI Platform Notebooks) restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmUZzSv6YA9-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  USER_FLAG = ''\n",
    "else:\n",
    "  USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGdU0lEfVwM-"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install {USER_FLAG} -U kfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IKZcgZnX3j6"
   },
   "outputs": [],
   "source": [
    "if not 'google.colab' in sys.modules:\n",
    "  # Automatically restart kernel after installs\n",
    "  import IPython\n",
    "  app = IPython.Application.instance()\n",
    "  app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mqs-ZFuW7bx"
   },
   "source": [
    "The KFP version should be >= 1.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4uvTyimMYOr"
   },
   "outputs": [],
   "source": [
    "# Check the KFP version\n",
    "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opiv8vjyKgBU"
   },
   "source": [
    "### Create and change to a new subdirectory\n",
    "\n",
    "We'll work in a `gcf` subdirectory for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcZkq7U8QZab"
   },
   "outputs": [],
   "source": [
    "!mkdir -p gcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2sFpGZHQZ5V"
   },
   "outputs": [],
   "source": [
    "%cd gcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tskC13YxW7b3"
   },
   "source": [
    "### Set some variables\n",
    "\n",
    "**Before you run the next cell**, **edit it** to set variables for your project.  \n",
    "\n",
    "For `WORKING_DIR`, enter the name of a Cloud Storage (GCS) path in your project. Include the `gs://` prefix. **The bucket must already exist**.  You can create a new bucket via the [Cloud Console](https://console.cloud.google.com/storage/browser) (or via the `gsutil mb` command-line utility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXvbCu2CYvoS"
   },
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Required Parameters\n",
    "WORKING_DIR = 'gs://your-gcs-path'  # <---CHANGE THIS\n",
    "\n",
    "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
    "REGION = 'us-central1' # <-----CHANGE THIS AS NECESSARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCNqNxgEhxCr"
   },
   "source": [
    "## Define and compile a pipeline\n",
    "\n",
    "Now we're set up to define, compile, and run a pipeline.  We'll use as a starting point the pipeline described [here](https://amygdala.github.io/gcp_blog/ml/kfp/mlops/keras/hp_tuning/2020/10/26/metrics_eval_component.html), but— for simplicity— without the Keras tuner part.\n",
    "\n",
    "That is, we'll use the training, eval, and serving components from that pipeline, and add two [TFDV](https://www.tensorflow.org/tfx/guide/tfdv)-related components— one to derive dataset statistics, and the other to detect drift between older and newer datasets using the derived statistical information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBC-TUrpTtlA"
   },
   "source": [
    "### Create the TFDV pipeline components\n",
    "\n",
    "We'll define both TFDV pipeline *components* as ['lightweight' Python-function-based components](https://www.kubeflow.org/docs/pipelines/sdk/python-function-components/). For each component, we define a function, then call `kfp.components.func_to_container_op()` on that function to build a reusable component in `.yaml` format. \n",
    "\n",
    "For these components, we need to specify a base container image that will run the function.  We'll use one that has the TFDV libraries installed (its Dockerfile is [here](https://github.com/amygdala/code-snippets/blob/keras_tuner3/ml/kubeflow-pipelines/keras_tuner/components/tfdv/Dockerfile))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJDhVEZJT8bW"
   },
   "source": [
    "#### Component to generate stats on the dataset\n",
    "\n",
    "This component uses TFDV to generate statistics on a given dataset.\n",
    "\n",
    "> Note: For this example, our training data is in GCS, in CSV-formatted files.  So, we can take advantage of TFDV’s ability to process CSV files.  The TFDV libraries can also process files in `TFRecords` format. \n",
    "\n",
    "It uses a [Beam](https://beam.apache.org/) pipeline— not to be confused with KFP Pipelines— to do this. Depending upon configuration, the component can use either the Direct (local) runner or the [Dataflow](https://cloud.google.com/dataflow#section-5) runner. \n",
    "\n",
    "Running the Beam pipeline on Dataflow rather than locally can make sense with large datasets. (If you use Dataflow, first ensure that the Datflow API is enabled for your GCP project).\n",
    "\n",
    "We'll first define the python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYrOzo4X0QLl"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def generate_tfdv_stats(input_data: str, output_path: str, job_name: str, use_dataflow: str,\n",
    "                        project_id: str, region:str, gcs_temp_location: str, gcs_staging_location: str,\n",
    "                        whl_location: str = '', requirements_file: str = 'requirements.txt'\n",
    ") -> NamedTuple('Outputs', [('stats_path', str)]):\n",
    "\n",
    "  import logging\n",
    "  import time\n",
    "\n",
    "  import tensorflow_data_validation as tfdv\n",
    "  import tensorflow_data_validation.statistics.stats_impl\n",
    "  from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n",
    "\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  logging.info(\"output path: %s\", output_path)\n",
    "  logging.info(\"Building pipeline options\")\n",
    "  # Create and set your PipelineOptions.\n",
    "  options = PipelineOptions()\n",
    "\n",
    "  if use_dataflow == 'true':\n",
    "    logging.info(\"using Dataflow\")\n",
    "    if not whl_location:\n",
    "      logging.warning('tfdv whl file required with dataflow runner.')\n",
    "      exit(1)\n",
    "    # For Cloud execution, set the Cloud Platform project, job_name,\n",
    "    # staging location, temp_location and specify DataflowRunner.\n",
    "    google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "    google_cloud_options.project = project_id\n",
    "    google_cloud_options.job_name = '{}-{}'.format(job_name, str(int(time.time())))\n",
    "    google_cloud_options.staging_location = gcs_staging_location\n",
    "    google_cloud_options.temp_location = gcs_temp_location\n",
    "    google_cloud_options.region = region\n",
    "    options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "    setup_options = options.view_as(SetupOptions)\n",
    "    # PATH_TO_WHL_FILE should point to the downloaded tfdv wheel file.\n",
    "    setup_options.extra_packages = [whl_location]\n",
    "    setup_options.requirements_file = 'requirements.txt'\n",
    "\n",
    "  tfdv.generate_statistics_from_csv(\n",
    "    data_location=input_data, output_path=output_path,\n",
    "    pipeline_options=options)\n",
    "\n",
    "  return (output_path, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecyngEySP0sF"
   },
   "source": [
    "This component outputs the path to the generated stats file. \n",
    "\n",
    "When we define the pipeline later in the lab, we'll see this output used as one of the inputs to the 'drift detection' component below.\n",
    "\n",
    "Next, we'll create the pipeline component from the function definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_GOGbeZ0ofX"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp.components.func_to_container_op(generate_tfdv_stats,\n",
    "    output_component_file='tfdv_component.yaml', base_image='gcr.io/google-samples/tfdv-tests:v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8aU-vBkUEZo"
   },
   "source": [
    "#### Detect dataset 'drift'\n",
    "\n",
    "Detect drift on the `duration` field between two sets of derived stats from two datasets.  See the [TFDV docs](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for more detail.  \n",
    "\n",
    "The component outputs a string indicating whether model drift was detected (it also returns 'true' if no comparison stats file was provided).  \n",
    "\n",
    "When we define the pipeline later in the lab, we'll see this output used in a *conditional* expression, to determine whether or not model training should occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMO_RycJ0RjA"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def tfdv_detect_drift(\n",
    "    stats_older_path: str, stats_new_path: str\n",
    ") -> NamedTuple('Outputs', [('drift', str)]):\n",
    "\n",
    "  import logging\n",
    "  import time\n",
    "\n",
    "  import tensorflow_data_validation as tfdv\n",
    "  import tensorflow_data_validation.statistics.stats_impl\n",
    "\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  logging.info('stats_older_path: %s', stats_older_path)\n",
    "  logging.info('stats_new_path: %s', stats_new_path)\n",
    "\n",
    "  # if there are no older stats to compare with, just return 'true'\n",
    "  if stats_older_path == 'none':\n",
    "    return ('true', )\n",
    "\n",
    "  stats1 = tfdv.load_statistics(stats_older_path)\n",
    "  stats2 = tfdv.load_statistics(stats_new_path)\n",
    "\n",
    "  schema1 = tfdv.infer_schema(statistics=stats1)\n",
    "  tfdv.get_feature(schema1, 'duration').drift_comparator.jensen_shannon_divergence.threshold = 0.01\n",
    "  drift_anomalies = tfdv.validate_statistics(\n",
    "      statistics=stats2, schema=schema1, previous_statistics=stats1)\n",
    "  logging.info('drift analysis results: %s', drift_anomalies.drift_skew_info)\n",
    "\n",
    "  from google.protobuf.json_format import MessageToDict\n",
    "  d = MessageToDict(drift_anomalies)\n",
    "  val = d['driftSkewInfo'][0]['driftMeasurements'][0]['value']\n",
    "  thresh = d['driftSkewInfo'][0]['driftMeasurements'][0]['threshold']\n",
    "  logging.info('value %s and threshold %s', val, thresh)\n",
    "  res = 'true'\n",
    "  if val < thresh:\n",
    "    res = 'false'\n",
    "  logging.info('train decision: %s', res)\n",
    "  return (res, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvtJxKnSP5R_"
   },
   "source": [
    "Create the component from the function definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZpxNn3q0X0j"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp.components.func_to_container_op(tfdv_detect_drift,\n",
    "    output_component_file='tfdv_drift_component.yaml', base_image='gcr.io/google-samples/tfdv-tests:v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSCh18iu4m0f"
   },
   "source": [
    "### Define the pipeline\n",
    "\n",
    "Now we've defined all the components we need, and are ready to define the pipeline, using the Kubeflow Pipelines (KFP) SDK. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJM9CAc6UpqQ"
   },
   "source": [
    "#### Define the pipeline ops from components\n",
    "\n",
    "We'll first define the pipeline ops we need from component definitions.\n",
    "\n",
    "In addition to the TFDV-based components that we just built, we'll use some other components that have already been created. These components train a model; perform a simple evaluation using model metrics; and deploy a model using TF-Serving.   \n",
    "See [this example](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md) for more information on the non-TFDV components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLWXsZ_XGMYS"
   },
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "# pre-existing components\n",
    "train_op = comp.load_component_from_url(\n",
    "  'https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/train_component.yaml'\n",
    "  )\n",
    "serve_op = comp.load_component_from_url(\n",
    "  'https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/serve_component.yaml'\n",
    "  )\n",
    "\n",
    "tb_op = comp.load_component_from_url(\n",
    "  'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/tensorflow/tensorboard/prepare_tensorboard/component.yaml'\n",
    "  )\n",
    "\n",
    "eval_metrics_op = comp.load_component_from_url(\n",
    "  'https://raw.githubusercontent.com/amygdala/code-snippets/master/ml/kubeflow-pipelines/keras_tuner/components/eval_metrics_component.yaml'\n",
    ")\n",
    "\n",
    "# our new TFDV components\n",
    "tfdv_op = comp.load_component_from_file(\n",
    "  'tfdv_component.yaml'\n",
    "  )\n",
    "tfdv_drift_op = comp.load_component_from_file(\n",
    "  'tfdv_drift_component.yaml'\n",
    "  )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlmr3grA4rJs"
   },
   "source": [
    "#### Define the pipeline using the defined ops\n",
    "\n",
    "Next, we'll define the pipeline itself, using the ops above.\n",
    "\n",
    "We'll specify two steps based on the TFDV stats-generation op— one run on the training data, and one run on the test data. The drift detection step consumes the stats generated for the training data. (The stats for the test data are not used by the downstream steps).\n",
    "\n",
    "Training is conditional on the results of the drift detection.\n",
    "\n",
    "Serving is conditional on the results of the model metrics at the end of training.\n",
    "\n",
    "**If you did not set up a GPU node pool on your GKE cluster, comment out the following line** in the definition below.  If you leave it uncommented, the 'train' step must be placed on a GPU-enabled node with at least two available GPUs (and will show status \"Pending\" until it can be placed— so if there are no GPU-enabled nodes in the cluster, it will wait 'forever').  \n",
    "```\n",
    "    train.set_gpu_limit(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ef8ESkkhwmq"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='bikes_weather',\n",
    "  description='Model bike rental duration given weather'\n",
    ")\n",
    "def bikes_weather_tfdv( \n",
    "  train_epochs: int = 3,\n",
    "  working_dir: str = 'gs://YOUR/GCS/PATH',  \n",
    "  data_dir: str = 'gs://aju-dev-demos-codelabs/bikes_weather/', # currently, requires trailing slash\n",
    "  steps_per_epoch: int = -1 ,  # if -1, don't override normal calcs based on dataset size\n",
    "  hptune_params: str = '[{\"num_hidden_layers\": %s, \"learning_rate\": %s, \"hidden_size\": %s}]' % (3, 1e-2, 64),\n",
    "  thresholds: str = '{\"root_mean_squared_error\": 2000}',\n",
    "  # tfdv-related\n",
    "  project_id: str = 'YOUR-PROJECT-ID',\n",
    "  region: str = 'us-central1',\n",
    "  requirements_file: str = 'requirements.txt',\n",
    "  job_name: str = 'testx',\n",
    "  whl_location: str = 'tensorflow_data_validation-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl',\n",
    "  use_dataflow: str = '',\n",
    "  stats_older_path: str = 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb'\n",
    "  ):\n",
    "\n",
    "\n",
    "  # create TensorBoard viz for the parent directory of all training runs, so that we can\n",
    "  # compare them.\n",
    "  tb_viz = tb_op(\n",
    "    log_dir_uri='%s/%s' % (working_dir, dsl.RUN_ID_PLACEHOLDER)\n",
    "  )\n",
    "\n",
    "  tfdv1 = tfdv_op(  # TFDV stats for the test data\n",
    "    input_data='%stest-*.csv' % (data_dir,),\n",
    "    output_path='%s/tfdv_expers/%s/eval/evaltest.pb' % (working_dir, dsl.RUN_ID_PLACEHOLDER),\n",
    "    job_name='%s-1' % (job_name,),\n",
    "    use_dataflow=use_dataflow,\n",
    "    project_id=project_id, region=region,\n",
    "    gcs_temp_location='%s/tfdv_expers/tmp' % (working_dir,), \n",
    "    gcs_staging_location='%s/tfdv_expers' % (working_dir,), \n",
    "    whl_location=whl_location, requirements_file=requirements_file\n",
    "    )\n",
    "  tfdv2 = tfdv_op(  # TFDV stats for the training data\n",
    "    input_data='%strain-*.csv' % (data_dir,),\n",
    "    # output_path='%s/%s/eval/evaltrain.pb' % (output_path, dsl.RUN_ID_PLACEHOLDER),\n",
    "    output_path='%s/tfdv_expers/%s/eval/evaltrain.pb' % (working_dir, dsl.RUN_ID_PLACEHOLDER),\n",
    "    job_name='%s-2' % (job_name,),\n",
    "    use_dataflow=use_dataflow,\n",
    "    project_id=project_id, region=region,\n",
    "    gcs_temp_location='%s/tfdv_expers/tmp' % (working_dir,), \n",
    "    gcs_staging_location='%s/tfdv_expers' % (working_dir,), \n",
    "    whl_location=whl_location, requirements_file=requirements_file\n",
    "    )\n",
    "\n",
    "  # compare generated training data stats with stats from a previous version\n",
    "  # of the training data set.\n",
    "  tfdv_drift = tfdv_drift_op(stats_older_path, tfdv2.outputs['stats_path'])\n",
    "  \n",
    "  # proceed with training if drift is detected (or if no previous stats were provided)\n",
    "  with dsl.Condition(tfdv_drift.outputs['drift'] == 'true'):  \n",
    "\n",
    "    train = train_op(\n",
    "      data_dir=data_dir,\n",
    "      workdir='%s/%s' % (tb_viz.outputs['log_dir_uri'], 0),\n",
    "      tb_dir=tb_viz.outputs['log_dir_uri'],\n",
    "      epochs=train_epochs, steps_per_epoch=steps_per_epoch,\n",
    "      hp_idx=0,\n",
    "      hptune_results=hptune_params\n",
    "      )\n",
    "\n",
    "    eval_metrics = eval_metrics_op(\n",
    "      thresholds=thresholds,\n",
    "      metrics=train.outputs['metrics_output_path'],\n",
    "      )\n",
    "\n",
    "    with dsl.Condition(eval_metrics.outputs['deploy'] == 'deploy'):\n",
    "      serve = serve_op(\n",
    "        model_path=train.outputs['train_output_path'],\n",
    "        model_name='bikesw',\n",
    "        namespace='default'\n",
    "        )\n",
    "    train.set_gpu_limit(2)  # comment out this line if you did not set up a GPU node pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4U2AE8ySIGJ"
   },
   "source": [
    "In the pipeline definition, you can see that the output of the `tfdv2` step (which generates stats on the training dataset) is consumed as an input by the `tfdv_drift` step.  Then, the output of that drift detection step is used in a conditional expression to decide whether or not model (re)training is necessary.\n",
    "\n",
    "(See [this example](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md) for more information on the non-TFDV parts of the pipeline.  You'll notice a second conditional expression, related to deciding whether or not a trained model should be deployed, based on its eval metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MSpouuXVBSv"
   },
   "source": [
    "### Compile the pipeline and (optionally) try a test run\n",
    "\n",
    "Now we're ready to compiile the pipeline, and try a test run of the pipeline to make sure that it is working correctly, before we set up the Cloud Function-based event triggering.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgPmUhfzA84h"
   },
   "outputs": [],
   "source": [
    "PIPELINE_SPEC = 'bw_train_tfdv.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQjUgcE491m-"
   },
   "source": [
    "Compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwJzSNqEiroj"
   },
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(bikes_weather_tfdv, PIPELINE_SPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-6nleYbluWD"
   },
   "source": [
    "To run the compiled pipeline from a notebook, we need to create a client object to connect to the pipelines installation on the GKE cluster.  To do this, we need to know the host URI. \n",
    "\n",
    "The host URI string can be obtained by clicking on the 'settings' gear for the relevant installation in the Cloud Console [dashboard](https://console.cloud.google.com/ai-platform/pipelines/clusters) that lists the pipelines installations, then copying the client connection info from the popup window.\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/kfp_host_uri.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/kfp_host_uri.png\" width=\"80%\"/></a>\n",
    "\n",
    "**Edit the following cell with your host URI before running it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNFHuDPIZD8K"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "PIPELINE_HOST = 'your-host-uri.pipelines.googleusercontent.com'  # CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-lJN9J-_dq3"
   },
   "outputs": [],
   "source": [
    "client = kfp.Client(host=PIPELINE_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y30GvgB8mIbx"
   },
   "source": [
    "Test the client connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5n_hgwTmEpl"
   },
   "outputs": [],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oncSyift2kq7"
   },
   "source": [
    "Create a KFP 'Experiment' under which to organize the pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T14JaLqmnmD"
   },
   "outputs": [],
   "source": [
    "exp = client.create_experiment(name='bw_expers')\n",
    "EXP_ID = exp.id\n",
    "print(EXP_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40bcq42r2qZ3"
   },
   "source": [
    "Upload the pipeline and get its ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQKAsb2uOm5H"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "ts = str(int(time.time()))\n",
    "res = client.upload_pipeline(pipeline_package_path=PIPELINE_SPEC, \n",
    "                                     pipeline_name='bw_metrics_tfdv_{}'.format(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qPrDsN6O750"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ID = res.id\n",
    "print(PIPELINE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2bLtwHPrDzt"
   },
   "source": [
    "If you view the uploaded pipeline details in the Kubeflow Pipelines dashboard, it should look like the following. You can see that the model training has a dependency on the training set drift analysis  (whereas TFDV stats for the test set are generated but not used further in this pipeline).\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline.png\" width=\"50%\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGmcmMELlE5e"
   },
   "source": [
    "Optionally, directly run the pipeline to test that it's working correctly.  In the cell below we're passing the pipeline ID, but we could alternately pass the compiled pipeline archive file.\n",
    "\n",
    "For the `data_dir` pipeline argument we're using the second half (chronologically) of the \"bike rental\" dataset, and as the `stats_older_path` argument we're using a statistics file previously generated from the first half of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ix136oz7yLk"
   },
   "outputs": [],
   "source": [
    "run = client.run_pipeline(EXP_ID, 'bw_tfdv_train', pipeline_id=res.id,\n",
    "                          params={'working_dir': WORKING_DIR,\n",
    "                                  'project_id': PROJECT_ID,\n",
    "                                  'use_dataflow': 'true', \n",
    "                                  'data_dir': 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2/',\n",
    "                                  'stats_older_path': 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXtEH2OVF5_k"
   },
   "source": [
    "View the running pipeline in the Console by clicking on the generated link above.  It will look something like this. (In this screenshot, the TFDV analysis steps have completed and the training step is in progress).\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline_run.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/bw_tfdv_pipeline_run.png\" width=\"70%\"/></a>\n",
    "\n",
    "This pipeline will take a while to run. In the meantime, proceed to the next section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iskhpQ-_-wwl"
   },
   "source": [
    "## Define and deploy a Cloud Function to launch a pipeline run\n",
    "\n",
    "Now we're ready to define and deploy a  [**Cloud Functions**](https://cloud.google.com/functions/docs/) (GCF) function that launches a run of this pipeline when new training data becomes available. \n",
    "\n",
    "In most cases, you don’t want to launch a new pipeline run for every new file added to a dataset— since typically, the dataset will be comprised of a collection of files, to which you will add/update multiple files in a batch. So, you don’t want the ‘trigger bucket’ to be the dataset bucket (if the data lives on GCS)— that will trigger unwanted pipeline runs.\n",
    "Instead, we’ll trigger a pipeline run after the upload of a *batch* of new data has completed.\n",
    "\n",
    "To do this, we’ll use an approach where the the 'trigger' bucket is different from the bucket used to store dataset files. ‘Trigger files’ uploaded to that bucket are expected to contain the path of the updated dataset as well as the path to the data stats file generated for the last model trained. \n",
    "A trigger file is uploaded once the new data upload has completed, and that upload triggers a run of the GCF function, which in turn reads info on the new data path from the trigger file and launches the pipeline job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jmGlg2Tg4_1"
   },
   "source": [
    "### Define the Cloud Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBjXCs6WRSje"
   },
   "source": [
    "First ensure that the Cloud Functions and Cloud Build APIs are enabled for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QORteHbq-WE"
   },
   "outputs": [],
   "source": [
    "!gcloud services enable cloudfunctions.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd2lO7yCOET-"
   },
   "source": [
    "Next, we'll write out a file that specifies the library installations required by the GCF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88Dy1_-lOa-1"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "kfp==1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDnfMaESObeO"
   },
   "source": [
    "Now we'll define the GCF function code in `main.py`.  \n",
    "\n",
    "Note that the function is grabbing some values from environment variables.  We'll show how those are set below.\n",
    "\n",
    "The `gcs_update` function will be called on the addition (or modification) of a file in the specified trigger bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IVXmGcF-KW_"
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp import components\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "PIPELINE_PROJECT_ID = os.getenv('PIPELINE_PROJECT_ID')\n",
    "REGION = 'us-central1'\n",
    "WORKING_DIR = os.getenv('WORKING_DIR')\n",
    "PIPELINE_SPEC = os.getenv('PIPELINE_SPEC')\n",
    "PIPELINE_ID = os.getenv('PIPELINE_ID')\n",
    "PIPELINE_HOST = os.getenv('PIPELINE_HOST')\n",
    "USE_DATAFLOW = os.getenv('USE_DATAFLOW')\n",
    "EXP_ID = os.getenv('EXP_ID')\n",
    "\n",
    "\n",
    "def read_trigger_file(data, context, storage_client):\n",
    "    \"\"\"Read the contents of the trigger file and return as string.\n",
    "    \"\"\"\n",
    "    print('Event ID: {}'.format(context.event_id))\n",
    "    print('Event type: {}'.format(context.event_type))\n",
    "    print('Data: {}'.format(data))\n",
    "    print('Bucket: {}'.format(data['bucket']))\n",
    "    print('File: {}'.format(data['name']))\n",
    "    print('Metageneration: {}'.format(data['metageneration']))\n",
    "    print('Created: {}'.format(data['timeCreated']))\n",
    "    print('Updated: {}'.format(data['updated']))\n",
    "\n",
    "    bucket = storage_client.get_bucket(data['bucket'])\n",
    "    blob = bucket.get_blob(data['name'])\n",
    "    trigger_file_string = blob.download_as_string().strip()\n",
    "    logging.info('trigger file contents: {}'.format(trigger_file_string))\n",
    "    return trigger_file_string.decode('UTF-8')\n",
    "\n",
    "\n",
    "def gcs_update(data, context):\n",
    "    \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n",
    "    \"\"\"\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    # get the contents of the trigger file\n",
    "    trigger_file_string = read_trigger_file(data, context, storage_client)\n",
    "    trigger_file_info = trigger_file_string.strip().split('\\n')  # TODO: add error-checking\n",
    "    logging.info('trigger file info: %s', trigger_file_info)\n",
    "    # then run the pipeline using the given job spec, passing the trigger file contents\n",
    "    # as parameter values.  \n",
    "    logging.info('running pipeline with id %s...', PIPELINE_ID)\n",
    "    # create the client object\n",
    "    client = kfp.Client(host=PIPELINE_HOST)    \n",
    "    # deploy the pipeline run\n",
    "    run = client.run_pipeline(EXP_ID, 'bw_tfdv_gcf', pipeline_id=PIPELINE_ID,\n",
    "                          params={'working_dir': WORKING_DIR,\n",
    "                                  'project_id': PIPELINE_PROJECT_ID,\n",
    "                                  'use_dataflow': USE_DATAFLOW,\n",
    "                                  'data_dir': trigger_file_info[0],\n",
    "                                  'stats_older_path': trigger_file_info[1]})     \n",
    "\n",
    "    logging.info('job response: %s', run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i0E3Q4TGi1o"
   },
   "source": [
    "Next, identify a GCS bucket in your project to use for the 'trigger bucket'.  This bucket must already exist.  You can create a new bucket via the [Cloud Console](https://console.cloud.google.com/storage/browser) (or via the `gsutil mb` command-line utility). **This bucket must be different from your `WORKING_DIR` bucket**.\n",
    "\n",
    "**Edit the following cell before running it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaR2RDpjgyVA"
   },
   "outputs": [],
   "source": [
    "# Change this to your bucket name.  Do not include the 'gs://' prefix.\n",
    "TRIGGER_BUCKET = 'your-bucket-name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGE-xuF6Vvz0"
   },
   "outputs": [],
   "source": [
    "# confirm that you're in the gcf subdirectory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ystw9qUIrmv"
   },
   "source": [
    "Now we're ready to deploy the GCF function code.  We specify to use the `gcs_update` definition (in the file `main.py`, which is implicit), and to use the value of the `TRIGGER_BUCKET` var as the trigger bucket.  The trigger event is specified to be the addition or modification of a GCS file in that bucket.\n",
    "\n",
    "Note that we're setting several environment variables as part of the deployment, to which the function code will have access.  These include the name of the pipeline job spec file, as well as the name of the Secret key that holds the API key string. So, ensure that all these vars are set properly— which they should be if you've run all the notebook cells above.\n",
    "\n",
    "The deployment will include the files in the current directory (`gcf`). The GCF function will have read-only access to the directory contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBC0Zqpq_l0P"
   },
   "outputs": [],
   "source": [
    "# You can also try setting to 'false', but this means that a Beam job runs locally on the GKE node\n",
    "# running the TFDV stats pipeline step (instead of launching a Dataflow job). \n",
    "# In that case, if your GKE node is not large enough, you may see an 'out of memory error'\n",
    "# when processing the training dataset.\n",
    "USE_DATAFLOW = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9XG_OXhA7Vf"
   },
   "source": [
    "Check that all the necessary vars are set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXPqZCqt_vgq"
   },
   "outputs": [],
   "source": [
    "print('PROJECT_ID {}, WORKING_DIR {}, PIPELINE_SPEC {}, PIPELINE_ID {}, \\nPIPELINE_HOST {}, EXP_ID {}, USE_DATAFLOW {}, TRIGGER_BUCKET {}'.format(\n",
    "    PROJECT_ID, WORKING_DIR, PIPELINE_SPEC, PIPELINE_ID, PIPELINE_HOST, EXP_ID, USE_DATAFLOW, TRIGGER_BUCKET\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxiNyka0PI_b"
   },
   "source": [
    "Then, deploy the GCF function.  Note how we're setting environment vars as part of the deployment.  Note also that we’re indicating to use the `gcs_update` definition (from `main.py`, which is implicit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fltAbzRyGlU9"
   },
   "outputs": [],
   "source": [
    "!gcloud functions deploy gcs_update --set-env-vars \\\n",
    "  PIPELINE_PROJECT_ID={PROJECT_ID},WORKING_DIR={WORKING_DIR},PIPELINE_SPEC={PIPELINE_SPEC},PIPELINE_ID={PIPELINE_ID},PIPELINE_HOST={PIPELINE_HOST},EXP_ID={EXP_ID},USE_DATAFLOW=true \\\n",
    "  --runtime python37 --trigger-resource {TRIGGER_BUCKET} --trigger-event google.storage.object.finalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjvcBwUvgxjk"
   },
   "source": [
    "You can see your deployed Cloud Function in the Console: https://console.cloud.google.com/functions/list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE4HQyGAg-ML"
   },
   "source": [
    "### Trigger the Cloud Function to run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMTgl1Ig_Zzb"
   },
   "source": [
    "\n",
    "Once deployment has completed, we're ready to test triggering the GCF function.  To do this, we'll upload a new file to the `TRIGGER_BUCKET`.\n",
    "\n",
    "We'll create a file that contains as its first line a path the new dataset to be processed.  \n",
    "Its second line contains either the path to TFDV stats for the dataset used for the previously-trained model (which we'll compare with the stats for the new data), or 'none'.\n",
    "\n",
    "Uploading this file will trigger a run of the GCF function we defined.\n",
    "\n",
    "The GCF function will read the contents of the trigger file, and kick off a pipeline run, passing the contents of the 'trigger file' as parameters to the pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqp8-B9N3sFd"
   },
   "source": [
    "For this simple example, we're using the two halves, chronologically, of our 'bike rental' dataset, for the 'old' and 'new' datasets.\n",
    "\n",
    "`gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds1/` is the first half, chronologically, of the 'bikes rental' dataset.  If we had wanted to trigger a pipeline run on that data, we could do it as per the commented-out cells below.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb1FJ2SWbv_f"
   },
   "outputs": [],
   "source": [
    "# %%writefile temp1.txt\n",
    "# gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds1/\n",
    "# none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obl5WMy7_nlZ"
   },
   "outputs": [],
   "source": [
    "#!gsutil cp temp1.txt gs://{TRIGGER_BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhFU9-Akg0EM"
   },
   "source": [
    "Instead, we'll skip that and jump ahead to processing the second dataset, comparing it to the stats from the first dataset to detect data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Yi0wiHRmri"
   },
   "source": [
    "In the following, `gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2` is the second half, chronologically, of the 'bikes and weather' data. \n",
    "\n",
    "The `.pb` file in the 2nd line points to the stats generated on the training set from the first half of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOYqN9bx_due"
   },
   "outputs": [],
   "source": [
    "%%writefile temp2.txt\n",
    "gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2/\n",
    "gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTvW6C6e4NYD"
   },
   "source": [
    "Upload `temp2.txt` to your 'trigger bucket' to trigger a run of the GCF function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ndTR5sIcjU3"
   },
   "outputs": [],
   "source": [
    "!gsutil cp temp2.txt gs://{TRIGGER_BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIO0I23Fdp0m"
   },
   "source": [
    "The upload will trigger a run of the GCF function, which in turn will trigger a pipeline run.  From the 'trigger file' the GCF function will obtain the path to the new input data \n",
    "(`gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2/`) as well as the path to the previously-generated stats.\n",
    "\n",
    "That is, the pipeline will compare the stats on the new training data to that of `gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb`, stats generated from the `gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds1/` dataset.  If drift is detected, model retraining will be initiated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hBC4nSX_zb6"
   },
   "source": [
    "\n",
    "You should be able to see the newly triggered pipeline job by visiting the Kubeflow Pipelines dashboard.  If you don't already have a tab open, it can be reached by clicking the \"**OPEN PIPELINES DASHBOARD**\" link next to your KFP installation in the [Cloud Console](https://console.cloud.google.com/ai-platform/pipelines/).\n",
    "\n",
    "If the pipeline doesn't seem to be running correctly, or if you want more info about how the GCF function was executed, you can \n",
    "view the GCF logs via [Cloud Logging](https://console.cloud.google.com/logs/viewer?resource=cloud_function) in the Console.  Select \"Cloud Function\" as the resource to view.\n",
    "From the log output of your function, you should see indication of the trigger file being read, and then the Pipeline run initiated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJeZr7vz4Pwz"
   },
   "source": [
    "## Appendix: Using your trained and deployed model(s) for prediction\n",
    "\n",
    "(Optional).\n",
    "\n",
    "Each time a new model is trained, it is deployed to the cluster using TF-Serving.  Once deployed, you can send prediction requests to the serving endpoint.  See [this example](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/keras_tuner/README.md#5-make-predictions-against-your-trained-model) for more detail on how to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89fYarRLW7cN"
   },
   "source": [
    "-----------------------------\n",
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hosted_event_triggered_pipeline_bw.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
